# Introduction to Information Theory
## 1.1 Random Variables

discrete random variable: X
probability distribution: $P_x(x)$

expectation value $Ef=\sum\limits_{x \in\chi}^n P_x(x)f(x)$

continuous random variable
$dP_x(x)$ : probability measure for an infinitesimal element

The total probability $P(X\in\alpha)$ that the variable X takes value in some(measurable) set $\alpha\in\chi$ is given by the integral
$P(X\in\alpha) = \int\limits_{x\in\alpha} dP_x(x)=\int I(x\in\alpha)dp_x(x)$

indication function $I(s)$: defined to be equal to 1 if the statement s is true, and equal to 0 if the statement is false

$Ef(x)=\int f(x)dp_x(x)$

$Var f(x)=E[f(x)^2]-[Ef(x)]^2$

## 1.2 Entropy





```python

```
